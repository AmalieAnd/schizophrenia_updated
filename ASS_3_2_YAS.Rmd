---
title: "Assignment 3 - Part 2 - Diagnosing Schizophrenia from Voice"
author: "Riccardo Fusaroli"
date: "October 17, 2017"
output: html_document
---

```{r setup, include=FALSE}
setwd("~/Library/Mobile Documents/com~apple~CloudDocs/4. Aarhus Universitet/3. Semester/1. Experimental Methods lll/R FOLDER/Schizophrenia_updated/Schizophrenia_updated")

# Libraries
library(pacman)
p_load(lmerTest, pastecs, ggplot2, tidyverse, gdata, MuMIn, effects, stringr, plyr, Metrics, vtreat, cvTools, simr,stringi, e1071, caret,sjPlot, boot)
```

```{r Data setup}
# Loading txt file
txt <- read.delim("ASS_3_2VoiceSchizo_Articulation.txt", sep =",")
# Loading data
df <- read.csv("data_with_demo.csv")

# Omitting NA's
df <- df[-c(1102,1103,1104,1105,1106,1107,1108,1109,1324,1325,1326,1327,1328,1329,1330,1331),]

#Removing "_"
df$trial <- str_remove(df$trial, "_")
df$ID <- as.numeric(as.factor(df$ID))
```


## Assignment 3 - Diagnosing schizophrenia from voice

In the previous part of the assignment you generated a bunch of "features", that is, of quantitative descriptors of voice in schizophrenia, focusing on pitch.
In the course of this assignment we will use them to try to automatically diagnose schizophrenia from voice only, that is, relying on the set of features you produced last time, we will try to produce an automated classifier.


### Question 1: Can you diagnose schizophrenia from pitch range only? If so, how well?

Build a logistic regression to see whether you can diagnose schizophrenia from pitch range only.
```{r }
df$diagnosis <- as.factor(df$diagnosis)

# Rescaling range
for (i in 9){
  minc = min(df[,i])
  maxc = max(df[,i])
  df[,i] = (df[,i]-minc)/(maxc-minc)
}

# Rescaling all variables
for (i in 6:13) {
  minc = min(df[,i])
  maxc = max(df[,i])
  df[,i] = (df[,i]-minc)/(maxc-minc)
}


# Creating model
model <- glmer(diagnosis ~ range + (1|ID) + (1|trial), df, family = binomial)
summary(model)
#modelgender <- glmer(Diagnosis~range + (1|ID) + (1|trial)+(1|Gender), df, family = binomial)
#summary(model)
#anova(model,modelgender)

# Plotting
ggplot(df, aes(range,diagnosis,colour=diagnosis)) +
  geom_point() + 
  theme_classic() 

# SJ plot
p_load(TMB)
sjPlot::sjp.glmer(model, type = 'fe.prob')

```
Calculate the different performance measures (accuracy, sensitivity, specificity, PPV, NPV, ROC curve) on a logistic regression using the full dataset. Don't forget the random effects!

Confusion Matrix
```{r Confusion matrix}
# Confusion matrix
df$PredictionsPerc=predict(model, type = "response")
df$Predictions[df$PredictionsPerc>0.5]="Control"
df$Predictions[df$PredictionsPerc<=0.5]="Schizophrenia"
df$Predictions <- as.factor(df$Predictions)
confusionMatrix(df$Predictions, reference = df$diagnosis, positive = "Schizophrenia")

```

ROC Curve
```{r ROC curve}
# ROC-cruve
p_load(pROC)
rocCurve <- roc(response = df$diagnosis,predictor = df$PredictionsPerc)
auc(rocCurve) 
# = 0.68

# Area under the curve = 68 % (1 = perfect, 0.5 = chance)
auc(rocCurve)
ci (rocCurve)
plot(rocCurve, legacy.axes = TRUE)
```

Then cross-validate the logistic regression and re-calculate performance on the testing folds. N.B. The cross-validation functions you already have should be tweaked: you need to calculate these new performance measures.

N.B. the predict() function generates log odds (the full scale between minus and plus infinity). Log odds > 0 indicates a choice of 1, below a choice of 0.
N.B. you need to decide whether calculate performance on each single test fold or save all the prediction for test folds in one datase, so to calculate overall performance.
N.B. Now you have two levels of structure: subject and study. Should this impact your cross-validation?

```{r Cross Validation}
set.seed(32)
folds = createFolds(unique(df$ID), 5)

#Preparing the variables
n = 1
accuracy = NULL
specificity = NULL
sensitivity = NULL
ppv = NULL
npv = NULL
auc = NULL

set.seed(666)


#Loop for loops <3 
for (i in folds){
  
  #Training data set
  train = subset(df,! (ID %in% i))
  
  #Test data set
  test = subset (df, (ID %in% i)) 
  
  #Creating the model on train data sat
  model=glmer(diagnosis ~ range + (1|trial)+ (1|ID), train, family = "binomial")
  
  #Predict on the test data, evaluation
  
  test$PredictionsPerc=predict(model, test, allow.new.levels = TRUE, type = "response") 
  test$Predictions[test$PredictionsPerc>0.5]="Schizophrenia" 
  test$Predictions[test$PredictionsPerc<=0.5]="Control"
  test$Predictions <- as.factor(test$Predictions)
  
  #Confusion matrix
    cm=confusionMatrix(data = test$Predictions, reference = test$diagnosis, positive = "Schizophrenia") 
  
  #Extracting the relevant features from the confusion matrix
  accuracy[n] = cm$overall["Accuracy"]
  
  sensitivity[n] =  sensitivity(data = test$Predictions, reference = test$diagnosis, positive = "Schizophrenia" )
  
  specificity [n] = specificity(data = test$Predictions, reference = test$diagnosis, negative = "Control" ) 
  
  ppv[n] = posPredValue(data = test$Predictions, reference = test$diagnosis, positive = "Schizophrenia") 
  
  npv[n] = negPredValue(data = test$Predictions, reference = test$diagnosis, negative = "Control")  
    #Making a ROC curve and extracting AUC
  rocCurve = roc(response = test$diagnosis,   predictor = test$PredictionsPerc) 
  auc[n]=auc(rocCurve) 
    
  n=n+1  
}

#Creating df with the data from the 5 fold CV
range_test = data.frame(accuracy, sensitivity, specificity, ppv, npv, auc)


#Taking the mean of relevant features
mean(range_test$auc)
mean(range_test$accuracy) 
mean(range_test$sensitivity) 
mean(range_test$specificity) 
```


### Question 2 - Which single acoustic predictor is the best predictor of diagnosis?
```{r Model comparison}
### First checking p-values for models to see if they're significant - if not, then not included in cross validation later
#Creating models

model_mean <- glmer(diagnosis~mean + (1|ID) + (1|trial), df, family = binomial)
model_median <- glmer(diagnosis~median + (1|ID) + (1|trial), df, family = binomial)
model_sd <- glmer(diagnosis~sd + (1|ID) + (1|trial), df, family = binomial)
model_range <- glmer(diagnosis~range + (1|ID) + (1|trial), df, family = binomial)
model_mad <- glmer(diagnosis~mean_abs + (1|ID) + (1|trial), df, family = binomial)
model_iqr <- glmer(diagnosis~iqr + (1|ID) + (1|trial), df, family = binomial)
model_se <- glmer(diagnosis~se + (1|ID) + (1|trial), df, family = binomial)
model_coefvar <- glmer(diagnosis~coef_var + (1|ID) + (1|trial), df, family = binomial)

#Summary
summary(model_range)

# Model comparison
anova(model_mean, model_median, model_sd, model_range, model_mad, model_iqr, model_se, model_coefvar)
```

Cross validation loop
```{r CV loop}
#Function taking data and model
Cross.dat.plz = function(data, model) {

#Preparing the variables
accuracy = NULL
specificity = NULL
sensitivity = NULL
ppv = NULL
npv = NULL
auc = NULL
n = 1


# Loop for loops <3
for (i in folds){
  
  # Train data set
  train = subset(df,! (ID %in% i))  
  
  # Test data set
  test = subset(df, (ID %in% i)) 
  
  # Model to use
  modellos = glmer(model, train, family = "binomial")
  
  # Predict on test data set
  test$PredictionsPerc=predict(modellos, test, allow.new.levels = TRUE, type = "response") 
  test$Predictions[test$PredictionsPerc>0.5]="Schizophrenia" 
  test$Predictions[test$PredictionsPerc<=0.5]="Control"
  test$Predictions <- as.factor(test$Predictions)
  
  # Create confusion matrix
  cm=confusionMatrix(data = test$Predictions, reference = test$diagnosis, positive = "Schizophrenia") 
  
  # Extracting the relevant values
  accuracy[n] = cm$overall["Accuracy"]
  
  sensitivity[n] =  sensitivity(data = test$Predictions, reference = test$diagnosis, positive = "Schizophrenia" )
  
  specificity [n] = specificity(data = test$Predictions, reference = test$diagnosis, negative = "Control" ) 
  ppv[n] = posPredValue(data = test$Predictions, reference = test$diagnosis, positive = "Schizophrenia") 
  npv[n] = negPredValue(data = test$Predictions, reference = test$diagnosis, negative = "Control")  
  
  roc_curve = roc(response = test$diagnosis,   predictor = test$PredictionsPerc) 
  auc[n]=auc(roc_curve) 
    
  n=n+1  
}

#Return it in a dataframe
tosender = data.frame(accuracy, sensitivity, specificity, ppv, npv, auc)


return(tosender)
}
```

Now cross validating features 
```{r CV features}
# Rescaling all variables
for (i in 6:13){
  minc = min(df[,i])
  maxc = max(df[,i])
  df[,i] = (df[,i]-minc)/(maxc-minc)
}

mean_cross = Cross.dat.plz(df, diagnosis ~ mean + (1 |ID) + (1|trial))
median_cross = Cross.dat.plz(df, diagnosis ~ median + (1 |ID) + (1|trial))

#range_cross = Cross.dat.plz(df, diagnosis ~ mean + (1 |ID) + (1|trial))
iqr__cross = Cross.dat.plz(df, diagnosis ~ iqr + (1 |ID) + (1|trial))
mad_cross = Cross.dat.plz(df, diagnosis ~ mean_abs + (1 |ID) + (1|trial))
cov_cross = Cross.dat.plz(df, diagnosis ~ coef_var + (1 |ID) + (1|trial))
#cov fails to converge??

sd_cross = Cross.dat.plz(df, diagnosis ~ sd + (1 |ID) + (1|trial))
se_cross = Cross.dat.plz(df, diagnosis ~ se + (1 |ID) + (1|trial))

summary(mean_cross)

```


### Question 3 - Which combination of acoustic predictors is best for diagnosing schizophrenia?

Now it's time to go wild! Use all (voice-related) variables and interactions you can think of. Compare models and select the best performing model you can find.

Remember:
- Out-of-sample error crucial to build the best model!
- After choosing the model, send Malte and Riccardo the code of your model
```{r Model comparison}
#Trying out models 
combi1 <- glmer(diagnosis ~ range*mean + (1 | Gender) + (1|study), df, family = "binomial")
summary(combi1)
combi2 <- glmer(diagnosis ~ range*mean + study + (1|Gender), df, family = "binomial")
summary(combi2)
combi3 <- glmer(diagnosis ~ range*mean + study + (1|ID) + (1|trial), df, family = "binomial")
summary(combi3)
combi4 <- glmer(diagnosis ~ range*sd + study + (1|ID), df, family = "binomial")
summary(combi4)
combi5 <- glmer(diagnosis ~ range*sd + (1|ID) + (1|study), df, family = "binomial")
summary(combi5)
combi6<- glmer(diagnosis ~ range*mean_abs + (1|ID) + (1|study), df, family = "binomial")
summary(combi6)

#we forgot to set seed. 
#Cross validation
combi1_cross <- Cross.dat.plz(df, diagnosis ~ range*mean + (1 | Gender) + (1|study))
combi2_cross <- Cross.dat.plz(df, diagnosis ~ range*mean + study + (1|Gender))
combi3_cross <- Cross.dat.plz(df, diagnosis ~ range*mean + study + (1|ID) + (1|trial))
combi4_cross <- Cross.dat.plz(df, diagnosis ~ range*sd + study + (1|ID))
combi5_cross <- Cross.dat.plz(df, diagnosis ~ range*sd + (1|ID) + (1|study))
combi6_cross <- Cross.dat.plz(df, diagnosis ~ range*mean_abs + (1|ID) + (1|study))

#Mean AUC to decide on best model
mean(combi1_cross$auc)
mean(combi2_cross$auc)
mean(combi3_cross$auc)
mean(combi4_cross$auc)
mean(combi5_cross$auc)
mean(combi6_cross$auc)
#[1] 0.6078922
#[2] 0.6070459
#[3] 0.5967868
#[4] 0.5991628
#[5] 0.6029624
#[6] 0.5897355

#Extracting mean and sd fromt the best model
mean(combi1_cross$accuracy)
sd(combi1_cross$accuracy)
mean(combi1_cross$sensitivity)
sd(combi1_cross$sensitivity)
mean(combi1_cross$specificity)
sd(combi1_cross$specificity)
mean(combi1_cross$ppv)
sd(combi1_cross$ppv)
mean(combi1_cross$npv)
sd(combi1_cross$npv)
mean(combi1_cross$auc)
sd(combi1_cross$auc)

#Confusion matrix
#Predict in probabilities not in log odd (the type = "response")
df$PredictionsBest <- predict(combi1, type = "response")
df$PredictionsCombi[df$PredictionsBest> 0.5]= "Control"
df$PredictionsCombi[df$PredictionsBest<= 0.5]= "Schizophrenia"
df$PredictionsCombi <- as.factor(df$PredictionsCombi)

confusionMatrix(data = df$PredictionsCombi, reference = df$diagnosis, positive
= "Schizophrenia")

```

```{r Plots}
library(ggplot2)
df$diagnosis <- as.numeric(df$diagnosis)
df$diagnosis <- as.character(df$diagnosis)
df$diagnosis[df$diagnosis == "1"] <- 0
df$diagnosis[df$diagnosis == "2"] <- 1
df$diagnosis <- as.numeric(df$diagnosis)

binomial_smooth <- function(...) {
  geom_smooth(method = "glm", method.args = list(family = "binomial"), ...)
}

ggplot(df, aes(x=diagnosis, y=range)) + geom_point(aes(color = diagnosis))+
  binomial_smooth() + theme_classic()
```


### Question 4: Properly report the results

METHODS SECTION: how did you analyse the data? That is, how did you extract the data, designed the models and compared their performance?

RESULTS SECTION: can you diagnose schizophrenia based on voice? which features are used? Comment on the difference between the different performance measures.
